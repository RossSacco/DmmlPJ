{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3464baed",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c355ea",
   "metadata": {},
   "source": [
    "Il dataset iniziale è il dataset completo delle interviste effettuate a oltre 400000 utenti.  Visto l'obiettivo di questo studio, ossia una classificazione multiclasse del diabete, verranno escluse dal dataset tutte quelle domande non utili alla generalizzazione, come info personali identificative dell'individuo intervistato o i suoi possedimenti.\n",
    "\n",
    "Verranno inoltre eliminate tutte quelle colonne derivate.\n",
    "\n",
    "Il tutto è stato possibile farlo consultando il notebook relativo al dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924670a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('dmml_diabetes_db.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140db3d",
   "metadata": {},
   "source": [
    "Le colonne da eliminare sono state trovate tramite codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonne derivate da eliminare:\n",
    "dropped_cols = [\n",
    "    \"_RFHLTH\", \"_HCVU651\", \"_RFHYPE5\", \"_CHOLCHK\", \"_RFCHOL\", \"_MICHD\",\n",
    "    \"_LTASTH1\", \"_CASTHM1\", \"_ASTHMS1\", \"_DRDXAR1\", \"_PRACE1\", \"_MRACE1\",\n",
    "    \"_HISPANC\", \"_RACEG21\", \"_RACEGR3\", \"_RACE_G1\", \"_AGEG5YR\", \"_AGE65YR\",\n",
    "    \"_AGE80\", \"HTIN4\", \"HTM4\", \"WTKG3\", \"_RFBMI5\", \"_CHLDCNT\", \"_EDUCAG\",\n",
    "    \"_INCOMG\", \"_RFSMOK3\", \"DRNKANY5\", \"DROCDY3_\", \"_RFBING5\", \"_DRNKWEK\",\n",
    "    \"_RFDRHV5\", \"FTJUDA1_\", \"FRUTDA1_\", \"BEANDAY_\", \"GRENDAY_\", \"ORNGDAY_\",\n",
    "    \"VEGEDA1_\", \"_MISFRTN\", \"_MISVEGN\", \"_FRTRESP\", \"_VEGRESP\", \"_FRUTSUM\",\n",
    "    \"_VEGESUM\", \"_FRTLT1\", \"_VEGLT1\", \"_FRT16\", \"_VEG23\", \"_FRUITEX\",\n",
    "    \"_VEGETEX\", \"_TOTINDA\", \"MAXVO2_\", \"FC60_\", \"STRFREQ_\", \"PAMISS1_\",\n",
    "    \"_PAINDX1\", \"_PA150R2\", \"_PA300R2\", \"_PA30021\", \"_PASTRNG\", \"_PAREC1\",\n",
    "    \"_PASTAE1\", \"_LMTACT1\", \"_LMTWRK1\", \"_LMTSCL1\", \"_RFSEAT2\", \"_RFSEAT3\",\n",
    "    \"_AIDTST3\", \"_CHISPNC\", \"_BMI5\"\n",
    "]\n",
    "indices = [df.columns.get_loc(col) for col in dropped_cols if col in df.columns]\n",
    "print(indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9106cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.get_loc('WEIGHT2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34f71c",
   "metadata": {},
   "source": [
    "Tenendo BMI tolgo altezza e peso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fea25",
   "metadata": {},
   "source": [
    "colonne derivate + colonne non inerenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_indices = (\n",
    "    list(range(1, 14))    # 1–13\n",
    "    + list(range(16, 20))  # 16–19\n",
    "    + list(range(21, 25))  # 21–24\n",
    "    + list(range(55, 59))  # 55–58\n",
    "    + [64] + [65]          # 64, 65\n",
    "    + [197]                # 197\n",
    "    + list(range(222, 232))# 222–230\n",
    "    + [235]                # 235\n",
    "    + list(range(237, 251))# 237–250\n",
    "    + list(range(252, 258))# 252–257\n",
    "    + list(range(259, 263))# 259–262\n",
    "    + list(range(264, 268))# 264–267\n",
    "    + list(range(269, 294))# 269–293\n",
    "    + list(range(296, 298))# 296–297\n",
    "    + list(range(306, 308))# 306–307\n",
    "    + list(range(315, 327))# 315–326\n",
    "    + [329]                # 329\n",
    ")\n",
    "\n",
    "# Rimuovi eventuali duplicati e tieni gli indici validi\n",
    "drop_indices = sorted({i for i in drop_indices if 0 <= i < len(df.columns)})\n",
    "\n",
    "# Ottengo i nomi delle colonne e elimino\n",
    "cols_to_drop = [df.columns[i] for i in drop_indices]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Eliminate {len(cols_to_drop)} colonne.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f07cf",
   "metadata": {},
   "source": [
    "Accorpamento in una singola colonna per due variabili che riguardano la stessa domanda, effettuata su linea fissa e su linea mobile, che le differenzia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13be46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa il numero di colonne iniziali\n",
    "print(f\"Colonne iniziali: {df.shape[1]}\")\n",
    "\n",
    "# Definizione nomi colonne\n",
    "primary_col   = 'NUMADULT'\n",
    "secondary_col = 'HHADULT'\n",
    "new_col       = 'NUMADULT_2'\n",
    "\n",
    "# Controllo esistenza nuova colonna\n",
    "if new_col in df.columns:\n",
    "    print(f\"La colonna `{new_col}` esiste già. Operazione saltata.\")\n",
    "# Controllo esistenza colonne di origine\n",
    "elif not {primary_col, secondary_col}.issubset(df.columns):\n",
    "    missing = [c for c in (primary_col, secondary_col) if c not in df.columns]\n",
    "    print(f\"Impossibile creare `{new_col}`, mancano le colonne: {missing}\")\n",
    "else:\n",
    "    # Creazione nuova colonna con preferenza per primary_col\n",
    "    df[new_col] = df[primary_col].combine_first(df[secondary_col])\n",
    "    # Eliminazione colonne originali in-place\n",
    "    df.drop(columns=[primary_col, secondary_col], inplace=True)\n",
    "    print(f\"✅ `{new_col}` creata unendo `{primary_col}` e `{secondary_col}`.\")\n",
    "    \n",
    "# Info post-elaborazione\n",
    "print(f\"Colonne finali: {df.shape[1]}\")\n",
    "print(\"Verifica presenza colonne originali:\")\n",
    "print(f\"- `{primary_col}` presente? {primary_col in df.columns}\")\n",
    "print(f\"- `{secondary_col}` presente? {secondary_col in df.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75fd32",
   "metadata": {},
   "source": [
    "Butto via le colonne che contengono più del 30% di valori Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soglia di NaN (30%)\n",
    "threshold = 0.30\n",
    "# Calcola la percentuale di NaN per colonna\n",
    "nan_ratio = df.isna().mean()\n",
    "# Trova colonne da eliminare\n",
    "cols_to_drop = nan_ratio[nan_ratio > threshold].index.tolist()\n",
    "# Stampa e rimuovi\n",
    "if cols_to_drop:\n",
    "    print(\"Colonne eliminate per eccesso di NaN (> 30%):\")\n",
    "    for col in cols_to_drop:\n",
    "        print(f\"- {col}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "else:\n",
    "    print(\"Nessuna colonna supera il 30% di valori NaN.\")\n",
    "print(f\"Numero colonne attuali: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proviamo a droppare le righe che contengon valori nulli.\n",
    "print(f\"Righe originali: {df.shape[0]}\")\n",
    "df = df.dropna()\n",
    "print(f\"Righe dopo il drop: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b11c4a",
   "metadata": {},
   "source": [
    "Aggiusto la variabile target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droppo le righe che hanno diabete3 pari a 7 o 9 (non lo so/ nessuna risposta)\n",
    "before = df.shape[0]\n",
    "df = df[~df['DIABETE3'].isin([7, 9])]\n",
    "after = df.shape[0]\n",
    "\n",
    "mapping_DIABETE3 = {\n",
    "    1: 'Diabetes',\n",
    "    2: 'NoDiabetes',\n",
    "    3: 'NoDiabetes',\n",
    "    4: \"PreDiabetes\", \n",
    "}\n",
    "df['DIABETE3'] = df['DIABETE3'].map(mapping_DIABETE3)\n",
    "\n",
    "print(f\"Righe eliminate: {before - after}\")\n",
    "print(f\"Righe rimanenti: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e72479",
   "metadata": {},
   "source": [
    "Ci occupiamo adesso delle variabili la cui codifica va cambiata per migliorare l'interpretazione nei modelli.\n",
    "\n",
    "Le variabili relative al consumo di cibo verranno tutte converitite in consumo mensile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c60bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq_food_to_monthly(code):\n",
    "    # valori missing/refused\n",
    "    if code in (777, 999) or pd.isna(code):\n",
    "        return np.nan\n",
    "    # mai\n",
    "    if code == 555:\n",
    "        return 0.0\n",
    "    # volte per giorno → moltiplico 30 giorni\n",
    "    if 101 <= code <= 199:\n",
    "        return (code - 100) * 30.0\n",
    "    # volte per settimana → moltiplico 4.345 settimane \n",
    "    if 201 <= code <= 299:\n",
    "        return (code - 200) * 4.345\n",
    "    # meno di 1 al mese → stimo 0.5\n",
    "    if code == 300:\n",
    "        return 0.5\n",
    "    # volte per mese diretto\n",
    "    if 301 <= code <= 399:\n",
    "        return code - 300\n",
    "    # altrimenti missing\n",
    "    return np.nan\n",
    "\n",
    "for var in ['FRUITJU1','FRUIT1','FVBEANS','FVGREEN','FVORANG','VEGETAB1']:\n",
    "    df[f'{var}'] = df[var].apply(freq_food_to_monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strength_to_weekly(code):\n",
    "    if code in (777, 999) or pd.isna(code):\n",
    "        return np.nan\n",
    "    if code == 888:\n",
    "        return 0.0\n",
    "    if 101 <= code <= 199:\n",
    "        return code - 100\n",
    "    if 201 <= code <= 299:\n",
    "        # converto il valore mensile in frequenza settimanale\n",
    "        return (code - 200) / 4.345\n",
    "    return np.nan\n",
    "\n",
    "df['STRENGTH'] = df['STRENGTH'].apply(strength_to_weekly) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296906f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alco_days_to_weekly(code):\n",
    "    if code in (777, 999) or pd.isna(code):\n",
    "        return np.nan\n",
    "    if code == 888:\n",
    "        return 0.0\n",
    "    if 101 <= code <= 199:\n",
    "        # giorni a settimana\n",
    "        return code - 100\n",
    "    if 201 <= code <= 299:\n",
    "        # giorni in 30 giorni → giorni/settimana\n",
    "        return (code - 200) / 4.345\n",
    "    return np.nan\n",
    "\n",
    "df['ALCDAY5'] = df['ALCDAY5'].apply(alco_days_to_weekly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ab48b",
   "metadata": {},
   "source": [
    "Trasformo le colonne in binarie per migliore interpretazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BPHIGH4'].unique()\n",
    "print(df['BPHIGH4'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "print(df['PERSDOC2'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSDOC2'] = pd.Series(np.select(\n",
    "    [\n",
    "        df['PERSDOC2'].isin([1, 2]),  # rischio\n",
    "        df['PERSDOC2'] == 3           # no rischio\n",
    "    ],\n",
    "    [\n",
    "        1,  # rischio\n",
    "        0   # no rischio\n",
    "    ],\n",
    "    default=np.nan  # incerti, rifiutati, mancanti\n",
    "), index=df.index).astype('Int64')  # nullable integer per compatibilità con imputazione\n",
    "\n",
    "df['BPHIGH4'] = pd.Series(np.select(\n",
    "    [\n",
    "        df['BPHIGH4'].isin([1, 2, 4]),  # rischio\n",
    "        df['BPHIGH4'] == 3              # no rischio\n",
    "    ],\n",
    "    [\n",
    "        1,  # rischio\n",
    "        0   # no rischio\n",
    "    ],\n",
    "    default=np.nan  # incerti, rifiutati, mancanti\n",
    "), index=df.index).astype('Int64')  # nullable integer per compatibilità con imputazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591eb69c",
   "metadata": {},
   "source": [
    "Trasformo nelle colonne binarie la codifica di no (2) in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BLOODCHO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def to_binary(series, yes_value=1, no_value=2):\n",
    "    \"\"\"\n",
    "    Trasforma una Serie pandas con codifica 1=yes, 2=no in 0/1.\n",
    "    - yes_value  → 1\n",
    "    - no_value   → 0\n",
    "    - tutti gli altri valori → np.nan\n",
    "    \"\"\"\n",
    "    return series.map({yes_value: 1, no_value: 0}).astype('Int64')\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "binary_cols = ['HLTHPLN1','MEDCOST','BLOODCHO','TOLDHI2','CVDINFR4',\n",
    "               'CVDCRHD4','CVDSTRK3','ASTHMA3','CHCSCNCR','CHCOCNCR',\n",
    "               'CHCCOPD1','HAVARTH3','ADDEPEV2','CHCKIDNY','VETERAN3',\n",
    "               'INTERNET','QLACTLM2','USEEQUIP','BLIND','DECIDE',\n",
    "               'DIFFWALK','DIFFDRES','DIFFALON','SMOKE100','EXERANY2','FLUSHOT6',\n",
    "               'PNEUVAC3', 'HIVTST6'] \n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}'] = to_binary(df[col])\n",
    "        print(df[f'{col}'].unique())\n",
    "    else:\n",
    "        print(f\"Colonna `{col}` non trovata nel DataFrame. Operazione saltata.\")\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BLOODCHO'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c4d3d",
   "metadata": {},
   "source": [
    "In alcune variabili la codifica dei valori non lo so/ rifiutato è differente. I valori con questo significato verranno trasformati in valori Nan. La loro frequenza è bassa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei codici da trattare come NaN\n",
    "missing_codes = [7, 9]\n",
    "\n",
    "# Elenco delle colonne *da saltare* (quelle in cui quei codici non sono missing)\n",
    "exceptions = [\n",
    "    'PHYSHLTH', 'MENTHLTH', 'CHILDREN', 'NUMADULT_2',\n",
    "    '_STATE', 'EMPLOY1', 'INCOME2', '_RACE'\n",
    "]\n",
    "\n",
    "# Calcolo le colonne *su cui* voglio fare il replace\n",
    "cols_to_clean = [c for c in df.columns if c not in exceptions]\n",
    "\n",
    "# Applico il replace solo su quelle\n",
    "df[cols_to_clean] = df[cols_to_clean].replace(missing_codes, np.nan)\n",
    "\n",
    "\n",
    "#in employ1 e _race togliere solo il 9 \n",
    "\n",
    "#da income2, physhlth, menthlth,  togliere 77 e 99\n",
    "#in children 88 = 0 e togliere 99\n",
    "\n",
    "# 1) EMPLOY1 e _RACE: codice “9” → NaN, poi category\n",
    "df[['EMPLOY1','_RACE']] = (\n",
    "    df[['EMPLOY1','_RACE']]\n",
    "    .replace(9, np.nan)\n",
    ")\n",
    "\n",
    "# 2) INCOME2, PHYSHLTH, MENTHLTH: codici “77” e “99” → NaN\n",
    "#    - Income2 resta categorica\n",
    "#    - PHYSHLTH e MENTHLTH sono numeriche (giorni), quindi float\n",
    "df['INCOME2'] = (\n",
    "    df['INCOME2']\n",
    "    .replace([77,99], np.nan)\n",
    ")\n",
    "df[['PHYSHLTH','MENTHLTH']] = (\n",
    "    df[['PHYSHLTH','MENTHLTH']]\n",
    "    .replace({77: np.nan, 99: np.nan, 88: 0})\n",
    ")\n",
    "\n",
    "# 3) CHILDREN: codice “88” → 0 (nessun bambino), “99” → NaN\n",
    "df['CHILDREN'] = (\n",
    "    df['CHILDREN']\n",
    "    .replace({88: 0, 99: np.nan})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8908807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola la frazione di NaN per colonna e moltiplica per 100\n",
    "nan_perc = df.isna().mean() * 100\n",
    "\n",
    "# Stampa nome colonna e percentuale formattata\n",
    "for col, perc in nan_perc.items():\n",
    "    print(f\"{col}: {perc:.2f}% NaN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20e531",
   "metadata": {},
   "source": [
    "Cambiamo adesso il tipo delle variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790899e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "\n",
    "nominal_cols = [\n",
    "    '_STATE','SEX','MARITAL','EMPLOY1','_RACE', '_BMI5CAT'\n",
    "]\n",
    "\n",
    "# definiamo quali ordinali hanno codifica \"1 = best, ↑ = worse\"\n",
    "ordinal_asc = {\n",
    "    'GENHLTH':       [1,2,3,4,5],       # 1=Excellent … 5=Poor\n",
    "    'CHECKUP1':      [1,2,3,4,5,6,7,8], # \n",
    "    'CHOLCHK':       [1,2,3,4],         \n",
    "    '_AGE_G':        [1,2,3,4,5,6],     # 1=18–24 … 6=65+\n",
    "    '_PACAT1':       [1,2,3,4],         # 1=High active … 4=Inactive\n",
    "    'SEATBELT':      [1,2,3,4,5]        # 1=Always … 5=Never (invertito)\n",
    "}\n",
    "\n",
    "# e quali ordinali hanno codifica \"1 = worst, ↑ = better\" (e.g. lower code = peggiore)\n",
    "ordinal_desc = {\n",
    "    'EDUCA':         [6,5,4,3,2,1],     # 1=None … 6=Post-grad (invertito)\n",
    "    'INCOME2':       [8,7,6,5,4,3,2,1],     # 1=Less than $10K … 6=$75K or more\n",
    "    '_SMOKER3':      [4,3,2,1] ,         # 1=Current every day … 4=Never\n",
    "    'USENOW3':       [3,2,1]          # 1=Every day … 3=Not at all (invertito)\n",
    "}\n",
    "\n",
    "# 2) Cast nominali a 'category' (manteniamo i codici numerici)\n",
    "for col in nominal_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# 3) Cast ordinali a 'category' con ordered=True\n",
    "for col, cats in ordinal_asc.items():\n",
    "    cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "    df[col] = df[col].astype(cat_type)\n",
    "\n",
    "for col, cats in ordinal_desc.items():\n",
    "    cat_type = CategoricalDtype(categories=cats, ordered=True)\n",
    "    df[col] = df[col].astype(cat_type)\n",
    "\n",
    "# 4) Le vere variabili continue in float\n",
    "numeric_cols = [\n",
    "    'PHYSHLTH','MENTHLTH','CHILDREN','NUMADULT_2',\n",
    "    'ALCDAY5', 'FRUITJU1','FRUIT1', 'FVBEANS','FVGREEN',\n",
    "    'FVORANG','VEGETAB1',  'STRENGTH'\n",
    "]\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 5) Controllo finale\n",
    "print(\"Dtypes finali:\")\n",
    "print(df[nominal_cols + list(ordinal_asc) + list(ordinal_desc) + numeric_cols].dtypes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb322d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CHILDREN'].unique()\n",
    "df['NUMADULT_2'].unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
